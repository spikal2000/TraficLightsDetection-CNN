{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QULdj1i7e2u6"
      },
      "source": [
        "# **Import python libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qqFfc3gqedYq"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, datasets\n",
        "from PIL import Image\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPool2D, Dropout, Flatten, Dense\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZeGXrIbKesvX"
      },
      "source": [
        "# **`Load the Datset from Kaggle`**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5qHdyZzIez8q",
        "outputId": "4fabc0c6-5f93-43a8-df3e-1f1c25e9d6be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.5.15)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle) (2023.5.7)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.65.0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.1)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.26.16)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.0.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.4)\n",
            "Downloading gtsrb-german-traffic-sign.zip to /content/dataset\n",
            " 99% 607M/612M [00:03<00:00, 210MB/s]\n",
            "100% 612M/612M [00:03<00:00, 197MB/s]\n"
          ]
        }
      ],
      "source": [
        "!pip install kaggle\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "# Set up Kaggle API credentials\n",
        "os.environ['KAGGLE_USERNAME'] = 'spikal2000'\n",
        "os.environ['KAGGLE_KEY'] = '8974f6a7db0a94d996b1579c714faa18'\n",
        "\n",
        "# Download the dataset using Kaggle API\n",
        "dataset_name = 'meowmeowmeowmeowmeow/gtsrb-german-traffic-sign'\n",
        "output_dir = '/content/dataset'\n",
        "\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "os.chdir(output_dir)\n",
        "\n",
        "!kaggle datasets download -d $dataset_name\n",
        "\n",
        "# Unzip the downloaded dataset\n",
        "with zipfile.ZipFile(os.path.join(output_dir, dataset_name.split('/')[1] + '.zip'), 'r') as zip_ref:\n",
        "    zip_ref.extractall()\n",
        "\n",
        "# Continue with the rest of the code for data preprocessing, model training, and object detection\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbpTtWRte1d1"
      },
      "source": [
        "# **Initializing Datset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "CqVS3He_fSli"
      },
      "outputs": [],
      "source": [
        "class TrafficSignsDataset(Dataset):\n",
        "    def __init__(self, csv_file, root_dir, transform=None):\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      image_path = self.root_dir + '/' + self.data['Path'][idx]\n",
        "      image = Image.open(image_path).convert('RGB')\n",
        "      label = self.data['ClassId'][idx]  # Changed from 'class' to 'ClassId'\n",
        "      if self.transform:\n",
        "          image = self.transform(image)\n",
        "      return image, label\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "WaymK9arfb3G"
      },
      "outputs": [],
      "source": [
        "# Set the path to your CSV file and image folder\n",
        "csv_file_train = '/content/dataset/Train.csv'\n",
        "csv_file_test = '/content/dataset/Test.csv'\n",
        "\n",
        "image_folder = '/content/dataset'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "t7D1ofXefsDu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90b9ce1a-cedf-4072-fe16-9a23d24e96ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<__main__.TrafficSignsDataset object at 0x7fc326262200>\n",
            "<__main__.TrafficSignsDataset object at 0x7fc45c1cb7c0>\n"
          ]
        }
      ],
      "source": [
        "# Define the transformation to apply to the training data\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((64, 64)),  # Resize the images to a consistent size\n",
        "    transforms.ToTensor(),  # Converts the images to PyTorch tensors\n",
        "])\n",
        "\n",
        "# Create the dataset\n",
        "train_dataset = TrafficSignsDataset(csv_file=csv_file_train, root_dir=image_folder, transform=transform)\n",
        "\n",
        "print(train_dataset)\n",
        "test_dataset = TrafficSignsDataset(csv_file=csv_file_test, root_dir=image_folder, transform=transform)\n",
        "print(test_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dnov9pujXOT"
      },
      "source": [
        "# **Data Augmentation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Ie9lYmw5jXjD"
      },
      "outputs": [],
      "source": [
        "# Define the transformation to apply to the training data\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(64),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
        "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize the images\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "-Zs-oY0CjdqZ"
      },
      "outputs": [],
      "source": [
        "# Create the training dataset with augmentation\n",
        "train_dataset = TrafficSignsDataset(csv_file=csv_file_train, root_dir=image_folder, transform=transform_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "QdrwxcbcjhVY"
      },
      "outputs": [],
      "source": [
        "# For testing, we don't want data augmentation, but we need to normalize it in the same way as the training data\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.Resize((64, 64)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "MRwGEbUEjh0Y"
      },
      "outputs": [],
      "source": [
        "# Create the testing dataset without augmentation\n",
        "test_dataset = TrafficSignsDataset(csv_file=csv_file_test, root_dir=image_folder, transform=transform_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pki26G1pf3yq"
      },
      "source": [
        "# **Define the CNN model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "9wbS9w8zfzu1"
      },
      "outputs": [],
      "source": [
        "# Create a data loader to efficiently load the data during training\n",
        "batch_size = 80\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "num_classes = 43\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "UXV6eiZIgHAE"
      },
      "outputs": [],
      "source": [
        "# Define your CNN model\n",
        "class CNNModel(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(CNNModel, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=5, stride=1, padding=2)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.conv2 = nn.Conv2d(32, 32, kernel_size=5, stride=1, padding=2)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.maxpool1 = nn.MaxPool2d(kernel_size=2)\n",
        "        self.dropout1 = nn.Dropout(0.25)\n",
        "\n",
        "\n",
        "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.maxpool2 = nn.MaxPool2d(kernel_size=2)\n",
        "        self.conv4 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu4 = nn.ReLU()\n",
        "\n",
        "        self.fc1 = nn.Linear(64 * 16 * 16, 512)  # Adjusted dimensions\n",
        "        self.relu5 = nn.ReLU()\n",
        "        self.dropout2 = nn.Dropout(0.25)\n",
        "\n",
        "        self.dropout3 = nn.Dropout(0.5)  # Additional dropout layer\n",
        "\n",
        "\n",
        "        self.fc2 = nn.Linear(512, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.relu1(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.relu2(out)\n",
        "        out = self.maxpool1(out)\n",
        "        out = self.dropout1(out)\n",
        "\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.relu3(out)\n",
        "        out = self.conv4(out)\n",
        "        out = self.relu4(out)\n",
        "        out = self.maxpool2(out)\n",
        "        out = self.dropout2(out)  # Apply the additional dropout layer\n",
        "\n",
        "        out = out.view(out.size(0), -1)\n",
        "\n",
        "        out = self.fc1(out)\n",
        "        out = self.relu5(out)\n",
        "        out = self.dropout3(out)\n",
        "\n",
        "        out = self.fc2(out)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "U1O1GuZlgIyi"
      },
      "outputs": [],
      "source": [
        "# Create an instance of CNN model\n",
        "model = CNNModel(num_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "-IGtayplgOBR"
      },
      "outputs": [],
      "source": [
        "# Define loss function\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "y6424LqngPW5"
      },
      "outputs": [],
      "source": [
        "# Define your optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PBqyXeFVgQ3u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d15eb6ff-1c96-4f30-e160-6ff73bd4dbb3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 1/55\n",
            "Epoch 1/55, Loss: 2.4168477058410645, Validation Accuracy: 0.3446555819477435\n",
            "Starting epoch 2/55\n",
            "Epoch 2/55, Loss: 2.9086477756500244, Validation Accuracy: 0.43151227236737927\n",
            "Starting epoch 3/55\n",
            "Epoch 3/55, Loss: 1.5799264907836914, Validation Accuracy: 0.5368962787015044\n",
            "Starting epoch 4/55\n",
            "Epoch 4/55, Loss: 1.9327744245529175, Validation Accuracy: 0.5945368171021378\n",
            "Starting epoch 5/55\n",
            "Epoch 5/55, Loss: 1.8490434885025024, Validation Accuracy: 0.5954077593032462\n",
            "Starting epoch 6/55\n",
            "Epoch 6/55, Loss: 1.1004009246826172, Validation Accuracy: 0.6527315914489311\n",
            "Starting epoch 7/55\n",
            "Epoch 7/55, Loss: 1.0105395317077637, Validation Accuracy: 0.6591448931116389\n",
            "Starting epoch 8/55\n",
            "Epoch 8/55, Loss: 1.3721158504486084, Validation Accuracy: 0.6400633412509897\n",
            "Starting epoch 9/55\n",
            "Epoch 9/55, Loss: 1.6756683588027954, Validation Accuracy: 0.7001583531274743\n",
            "Starting epoch 10/55\n",
            "Epoch 10/55, Loss: 1.341078519821167, Validation Accuracy: 0.7246239113222486\n",
            "Starting epoch 11/55\n",
            "Epoch 11/55, Loss: 1.5590230226516724, Validation Accuracy: 0.7314330958036421\n",
            "Starting epoch 12/55\n",
            "Epoch 12/55, Loss: 1.2343002557754517, Validation Accuracy: 0.743784639746635\n",
            "Starting epoch 13/55\n",
            "Epoch 13/55, Loss: 0.5486371517181396, Validation Accuracy: 0.7593032462391133\n",
            "Starting epoch 14/55\n",
            "Epoch 14/55, Loss: 1.6357100009918213, Validation Accuracy: 0.7723673792557403\n",
            "Starting epoch 15/55\n",
            "Epoch 15/55, Loss: 0.7682878971099854, Validation Accuracy: 0.7634204275534442\n",
            "Starting epoch 16/55\n",
            "Epoch 16/55, Loss: 0.6660168170928955, Validation Accuracy: 0.7483768804433888\n",
            "Starting epoch 17/55\n",
            "Epoch 17/55, Loss: 0.7814892530441284, Validation Accuracy: 0.7860649247822644\n",
            "Starting epoch 18/55\n",
            "Epoch 18/55, Loss: 0.6660388708114624, Validation Accuracy: 0.7967537608867775\n",
            "Starting epoch 19/55\n",
            "Epoch 19/55, Loss: 0.787959635257721, Validation Accuracy: 0.7849564528899445\n",
            "Starting epoch 20/55\n",
            "Epoch 20/55, Loss: 0.5767942070960999, Validation Accuracy: 0.78756927949327\n",
            "Starting epoch 21/55\n",
            "Epoch 21/55, Loss: 0.6261640191078186, Validation Accuracy: 0.8079968329374505\n",
            "Starting epoch 22/55\n",
            "Epoch 22/55, Loss: 0.753784716129303, Validation Accuracy: 0.8015835312747427\n",
            "Starting epoch 23/55\n"
          ]
        }
      ],
      "source": [
        "# Train your model\n",
        "num_epochs = 55\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Starting epoch {epoch+1}/{num_epochs}\")\n",
        "    # Training loop\n",
        "    for images, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    validation_accuracy = correct / total\n",
        "    # Print training/validation metrics for the epoch\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}, Validation Accuracy: {validation_accuracy}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mDHyBGQcgW_z"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), 'trafic_signs_classification_2.pth')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "QULdj1i7e2u6",
        "ZeGXrIbKesvX",
        "dbpTtWRte1d1",
        "0dnov9pujXOT"
      ],
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}